# LLM Proxy Configuration Example

# Provider configurations
# New style: allows declaring arbitrary providers and their "format" will be either "openai" or "anthropic"
providers:
  anthropic:
    base_url: "https://api.anthropic.com"
    max_retries: 3
    format: "anthropic" # required

  zai:
    base_url: "https://api.z.ai/api/anthropic"
    api_key: "xxx"
    format: "anthropic" # required

  openai:
    api_key: "..."
    base_url: "https://api.openai.com"
    format: "openai" #required
    fallback_provider: "plano"  # Optional: fallback to Plano if OpenAI fails
    circuit_breaker:
      enabled: true
      max_failures: 5
      timeout: "30s"

  localllm:
    base_url: "localhost:1234"
    format: "openai" # required

  # Plano (formerly ArchGW) - Multi-LLM routing proxy
  # Plano routes requests to 11+ providers: Gemini, DeepSeek, Qwen, Mistral, Groq, etc.
  # See: https://planoai.dev and https://github.com/katanemo/archgw
  plano:
    base_url: "http://localhost:8080"  # Or http://plano:8080 in Docker
    format: "openai"  # Plano accepts OpenAI-format requests
    fallback_provider: "anthropic"  # Optional: fallback to Anthropic if Plano fails
    circuit_breaker:
      enabled: true
      max_failures: 3
      timeout: "60s"
    # Note: No api_key needed - Plano reads upstream API keys from environment:
    #   OPENAI_API_KEY, GEMINI_API_KEY, DEEPSEEK_API_KEY, DASHSCOPE_API_KEY, etc.

# Subagent Configuration
# New style: We must refer to the specific provider as well as the model
subagents:
  mappings:
    # <agent name>: "<provider name>:<model name>"

    # OpenAI routing
    code-reviewer: "openai:gpt-4o"

    # Local LLM routing
    planner: "localllm:my-local-agent"

    # Z.ai routing
    janitor: "zai:glm-4.6"

    # Plano routing - Access multiple providers through one proxy
    # Plano model format: "provider/model-name"
    # Examples:
    #   gemini/gemini-2.0-flash-exp    - Google Gemini (fast, experimental)
    #   deepseek/deepseek-chat         - DeepSeek (cost-effective)
    #   deepseek/deepseek-coder        - DeepSeek Coder (code-focused)
    #   qwen/qwen-max                  - Alibaba Qwen (general purpose)
    #   mistral/mistral-large-latest   - Mistral (high quality)
    #   groq/llama-3.3-70b-versatile   - Groq (fast inference)

    # Example Plano subagent mappings:
    # fast-helper: "plano:gemini/gemini-2.0-flash-exp"
    # code-generator: "plano:deepseek/deepseek-coder"
    # budget-agent: "plano:qwen/qwen-max"

# Routing Configuration (Phase 3: Preference-Based Routing & Load Balancing)
routing:
  # Default routing preference
  preferences:
    default: balanced  # Options: cost, speed, quality, balanced

  # Per-task routing preferences
  # Tasks can override the default preference and specify preferred providers
  tasks:
    code_generation:
      preference: quality
      providers: [deepseek, openai]  # Prefer these providers for code generation

    fast_responses:
      preference: speed
      providers: [gemini, groq]  # Prefer fast providers

    budget_tasks:
      preference: cost
      providers: [qwen, deepseek]  # Prefer cost-effective providers

  # Provider characteristics for routing decisions
  # Values are on a 1-10 scale (higher is better)
  provider_profiles:
    gemini:
      speed: 9    # Very fast
      cost: 7     # Cost-effective
      quality: 8  # High quality

    deepseek:
      speed: 7    # Good speed
      cost: 9     # Very cost-effective
      quality: 8  # High quality, especially for code

    openai:
      speed: 8    # Fast
      cost: 5     # More expensive
      quality: 9  # Very high quality

    qwen:
      speed: 7    # Good speed
      cost: 8     # Cost-effective
      quality: 7  # Good quality

    plano:
      speed: 7    # Depends on underlying provider
      cost: 8     # Generally cost-effective
      quality: 7  # Depends on underlying provider

# NOTE: OLD CONFIGS ARE NOT SUPPORTED.
