# LLM Proxy Configuration Example

# Server configuration
server:
  port: "3001"  # For monolith; split services use PROXY_CORE_PORT/PROXY_DATA_PORT env vars
  timeouts:
    read: "600s"
    write: "600s"
    idle: "600s"

# Storage configuration
# IMPORTANT: For split services, both proxy-core and proxy-data must access the same database.
# Use an absolute path or DATA_DIR environment variable to ensure consistency.
storage:
  # Option 1: Absolute path (recommended for production)
  # db_path: "/data/claude-proxy/requests.db"

  # Option 2: Relative path (default, works for local development)
  db_path: "requests.db"

  # Environment variable override: set DB_PATH to override this value
  # Example: DB_PATH=/data/requests.db ./bin/proxy-core

# Provider configurations
# New style: allows declaring arbitrary providers and their "format" will be either "openai" or "anthropic"
providers:
  anthropic:
    base_url: "https://api.anthropic.com"
    max_retries: 3
    format: "anthropic" # required

  zai:
    base_url: "https://api.z.ai/api/anthropic"
    api_key: "xxx"
    format: "anthropic" # required
  
  openai:
    api_key: "..."
    base_url: "https://api.openai.com"
    format: "openai" #required

  localllm:
    base_url: "localhost:1234"
    format: "openai" # required 

# Subagent Configuration
# New style: We must refer to the specific provider as well as the model
subagents:
  mappings:
    # <agent name>: "<provider name>:<model name>"
    code-reviewer: "openai:gpt-4o"
    planner: "localllm:my-local-agent"
    janitor: "zai:glm-4.6"

# NOTE: OLD CONFIGS ARE NOT SUPPORTED.