# LLM Proxy Configuration Example

# Provider configurations
# New style: allows declaring arbitrary providers and their "format" will be either "openai" or "anthropic"
providers:
  anthropic:
    base_url: "https://api.anthropic.com"
    max_retries: 3
    format: "anthropic" # required

  zai:
    base_url: "https://api.z.ai/api/anthropic"
    api_key: "xxx"
    format: "anthropic" # required

  openai:
    api_key: "..."
    base_url: "https://api.openai.com"
    format: "openai" #required
    circuit_breaker:
      enabled: true
      max_failures: 5
      timeout: "30s"

  # Google Gemini via OpenAI-compatible API
  # Get your API key from https://aistudio.google.com/apikey
  gemini:
    api_key: "${GEMINI_API_KEY}"
    base_url: "https://generativelanguage.googleapis.com/v1beta/openai/"
    format: "openai" # required - Gemini uses OpenAI-compatible format
    max_retries: 3
    # Available models:
    # - gemini-2.0-flash-exp  (fastest, experimental)
    # - gemini-1.5-pro        (most capable)
    # - gemini-1.5-flash      (balanced speed/capability)

  localllm:
    base_url: "localhost:1234"
    format: "openai" # required

# Subagent Configuration
# New style: We must refer to the specific provider as well as the model
subagents:
  mappings:
    # <agent name>: "<provider name>:<model name>"

    # OpenAI routing
    code-reviewer: "openai:gpt-4o"

    # Gemini routing - fast experimental model for code review
    fast-reviewer: "gemini:gemini-2.0-flash-exp"

    # Gemini routing - most capable model for complex tasks
    architect: "gemini:gemini-1.5-pro"

    # Local LLM routing
    planner: "localllm:my-local-agent"

    # Z.ai routing
    janitor: "zai:glm-4.6"

# Routing Configuration (Phase 3: Preference-Based Routing & Load Balancing)
routing:
  # Default routing preference
  preferences:
    default: balanced  # Options: cost, speed, quality, balanced

  # Per-task routing preferences
  # Tasks can override the default preference and specify preferred providers
  tasks:
    code_generation:
      preference: quality
      providers: [deepseek, openai]  # Prefer these providers for code generation

    fast_responses:
      preference: speed
      providers: [gemini, groq]  # Prefer fast providers

    budget_tasks:
      preference: cost
      providers: [qwen, deepseek]  # Prefer cost-effective providers

  # Provider characteristics for routing decisions
  # Values are on a 1-10 scale (higher is better)
  provider_profiles:
    gemini:
      speed: 9    # Very fast
      cost: 7     # Cost-effective
      quality: 8  # High quality

    deepseek:
      speed: 7    # Good speed
      cost: 9     # Very cost-effective
      quality: 8  # High quality, especially for code

    openai:
      speed: 8    # Fast
      cost: 5     # More expensive
      quality: 9  # Very high quality

    qwen:
      speed: 7    # Good speed
      cost: 8     # Cost-effective
      quality: 7  # Good quality

# NOTE: OLD CONFIGS ARE NOT SUPPORTED.
