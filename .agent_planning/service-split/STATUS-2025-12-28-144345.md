# Evaluation: Service Split for Zero-Downtime Architecture

**Timestamp:** 2025-12-28-144345
**Evaluator:** project-evaluator
**Scope:** Architecture analysis for splitting monolithic proxy into two services
**Git Commit:** c6b5c10

---

## Executive Summary

**Current State:** Single monolithic Go service (~7,000 LOC) handles both critical proxy operations and data processing/serving.

**Goal:** Split into two services:
1. **proxy-core** - Lightweight, stable proxy (rarely changes)
2. **proxy-data** - Data processing, indexing, and dashboard APIs (frequent changes)

**Verdict:** **FEASIBLE** with clear split points. The codebase has natural boundaries that make this separation clean. However, shared SQLite database creates coupling that needs addressing.

---

## Current Architecture Analysis

### What Exists Now

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Single Go Service                     â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Proxying   â”‚  â”‚    Data      â”‚  â”‚   Dashboard  â”‚  â”‚
â”‚  â”‚   /v1/msgs   â”‚  â”‚  Processing  â”‚  â”‚     APIs     â”‚  â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚
â”‚  â”‚ - Provider   â”‚  â”‚ - Indexer    â”‚  â”‚ - Stats      â”‚  â”‚
â”‚  â”‚   routing    â”‚  â”‚ - FTS5       â”‚  â”‚ - Search     â”‚  â”‚
â”‚  â”‚ - Streaming  â”‚  â”‚ - Conv parse â”‚  â”‚ - Requests   â”‚  â”‚
â”‚  â”‚ - Response   â”‚  â”‚              â”‚  â”‚              â”‚  â”‚
â”‚  â”‚   logging    â”‚  â”‚              â”‚  â”‚              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                 â”‚                 â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                           â”‚                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚   SQLite    â”‚                      â”‚
â”‚                    â”‚ requests.db â”‚                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Code Distribution

**Critical Path (proxy-core territory):**
- `/v1/messages` handler (handlers.go:52-140)
- Provider routing (model_router.go:169-230)
- Provider implementations (provider/*.go)
- Request/response streaming (handlers.go:521-769)
- Basic request logging (storage_sqlite.go:202-246)

**Data Processing (proxy-data territory):**
- Conversation indexer (indexer.go:1-357) - background goroutine
- FTS5 search (storage_sqlite.go:1337-1440)
- Statistics aggregation (storage_sqlite.go:742-1335)
- Dashboard APIs (handlers.go:192-1027, handlers_v2.go)

**Shared/Coupled:**
- SQLite storage layer (storage_sqlite.go)
- Configuration (config.go)
- Data models (models.go)

---

## Natural Split Points

### Clear Boundaries

âœ… **Clean separation exists:**

1. **HTTP Endpoints** - Zero overlap
   - proxy-core: `/v1/messages`, `/v1/models`, `/health`
   - proxy-data: All `/api/*` endpoints

2. **Background Work** - Orthogonal concerns
   - proxy-core: None (purely request/response)
   - proxy-data: Conversation indexer, FTS5 updates

3. **Provider Integration** - Isolated to proxy-core
   - All provider code (anthropic.go, openai.go, provider.go)
   - Model routing logic
   - Format conversion

### Coupled Components

âŒ **Tight coupling exists:**

1. **SQLite Database** - Single point of contention
   - Both services need access
   - proxy-core: Writes raw request/response
   - proxy-data: Reads for aggregation, writes for indexing

2. **Data Models** - Shared structs
   - `model.RequestLog`, `model.AnthropicRequest`, etc.
   - Both services need identical definitions

3. **Configuration** - Currently unified
   - Provider configs needed by proxy-core
   - Storage config needed by both

---

## Proposed Architecture

### Option A: Shared Database (Simplest)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    proxy-core        â”‚          â”‚    proxy-data        â”‚
â”‚  (Port 3001)         â”‚          â”‚  (Port 3002)         â”‚
â”‚                      â”‚          â”‚                      â”‚
â”‚  /v1/messages        â”‚          â”‚  /api/*              â”‚
â”‚  /v1/models          â”‚          â”‚  /api/v2/*           â”‚
â”‚  /health             â”‚          â”‚                      â”‚
â”‚                      â”‚          â”‚  Background:         â”‚
â”‚  Providers:          â”‚          â”‚  - Indexer           â”‚
â”‚  - Anthropic         â”‚          â”‚  - FTS5 updates      â”‚
â”‚  - OpenAI            â”‚          â”‚                      â”‚
â”‚  - Model routing     â”‚          â”‚                      â”‚
â”‚                      â”‚          â”‚                      â”‚
â”‚  Writes:             â”‚          â”‚  Reads/Writes:       â”‚
â”‚  - requests table    â”‚          â”‚  - requests table    â”‚
â”‚    (raw logs)        â”‚          â”‚  - conversations     â”‚
â”‚                      â”‚          â”‚  - conversations_fts â”‚
â”‚                      â”‚          â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                 â”‚
           â”‚         Shared SQLite           â”‚
           â”‚       (requests.db)             â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                 â”‚  SQLite DB  â”‚
                 â”‚  WAL mode   â”‚
                 â”‚  (concurrent)â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- Simplest implementation
- SQLite WAL mode supports concurrent readers + 1 writer
- No network communication between services
- Data consistency is automatic

**Cons:**
- Still coupled via shared file
- Both services must run on same host
- Can't scale services independently to different machines
- Database file locking could be a bottleneck under extreme load

**Risk Assessment:**
- **Low risk** for current scale (single-user proxy)
- SQLite with WAL handles this well (used by production systems)
- Write contention unlikely (proxy writes ~1-10 req/sec, indexer batch writes)

---

### Option B: Message Queue Decoupling (Future-Proof)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    proxy-core        â”‚          â”‚    proxy-data        â”‚
â”‚  (Port 3001)         â”‚          â”‚  (Port 3002)         â”‚
â”‚                      â”‚          â”‚                      â”‚
â”‚  /v1/messages   â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”    â”‚  /api/*              â”‚
â”‚  /v1/models          â”‚      â”‚    â”‚  /api/v2/*           â”‚
â”‚  /health             â”‚      â”‚    â”‚                      â”‚
â”‚                      â”‚      â”‚    â”‚  Background:         â”‚
â”‚  Providers:          â”‚      â”‚    â”‚  - Indexer           â”‚
â”‚  - Anthropic         â”‚      â”‚    â”‚  - FTS5 updates      â”‚
â”‚  - OpenAI            â”‚      â”‚    â”‚  - Queue consumer    â”‚
â”‚  - Model routing     â”‚      â”‚    â”‚                      â”‚
â”‚                      â”‚      â”‚    â”‚                      â”‚
â”‚  Writes:             â”‚      â”‚    â”‚  Reads/Writes:       â”‚
â”‚  - Enqueues event    â”‚â—„â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€ Shared queue       â”‚
â”‚    (async)           â”‚      â”‚    â”‚  - Own SQLite DB     â”‚
â”‚                      â”‚      â”‚    â”‚    (full control)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Message Queue â”‚
                      â”‚  (SQLite/Redis)â”‚
                      â”‚                â”‚
                      â”‚  Events:       â”‚
                      â”‚  - request_log â”‚
                      â”‚  - response    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- True decoupling - services can restart independently
- Can run on different machines
- proxy-core becomes even lighter (no DB writes)
- Back-pressure handling (queue buffers if proxy-data is down)
- Can scale proxy-data horizontally

**Cons:**
- More complexity (queue management, durability, retry logic)
- Eventual consistency (stats lag slightly behind real requests)
- Need to handle queue failures, full queues, message ordering

**Risk Assessment:**
- **Higher complexity** than needed for current scale
- Worth it if planning multi-host deployment
- Consider for v2, not v1

---

### Option C: HTTP API Between Services (Middle Ground)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    proxy-core        â”‚          â”‚    proxy-data        â”‚
â”‚  (Port 3001)         â”‚          â”‚  (Port 3002)         â”‚
â”‚                      â”‚          â”‚                      â”‚
â”‚  /v1/messages   â”€â”€â”€â”€â”¼â”€HTTP POSTâ”€â”¤ POST /internal/log   â”‚
â”‚  /v1/models          â”‚  async    â”‚                      â”‚
â”‚  /health             â”‚           â”‚  /api/*              â”‚
â”‚                      â”‚           â”‚  /api/v2/*           â”‚
â”‚  Providers:          â”‚           â”‚                      â”‚
â”‚  - Anthropic         â”‚           â”‚  Background:         â”‚
â”‚  - OpenAI            â”‚           â”‚  - Indexer           â”‚
â”‚  - Model routing     â”‚           â”‚  - FTS5 updates      â”‚
â”‚                      â”‚           â”‚                      â”‚
â”‚  Zero DB writes      â”‚           â”‚  Owns data:          â”‚
â”‚  (fully stateless)   â”‚           â”‚  - SQLite DB         â”‚
â”‚                      â”‚           â”‚  - Full control      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pros:**
- Clean service boundary (HTTP contract)
- proxy-core is **completely stateless** (pure proxy)
- Can run on different machines
- Simple retry/timeout logic
- Easy to test in isolation

**Cons:**
- Network overhead for every request
- Need to handle proxy-data downtime (what if it's unreachable?)
- Adds latency to proxy path (~1-5ms local HTTP call)

**Risk Assessment:**
- **Good balance** for current needs
- Clearly defines service contract
- Can add async queue later if HTTP becomes bottleneck

---

## Recommended Split Strategy

### Phase 1: Shared Database Split (RECOMMENDED START)

**Why start here:**
1. Lowest risk, fastest to implement
2. Proves the separation works
3. Can migrate to Option B/C later if needed
4. Minimal code changes

**Implementation steps:**

1. **Create two binaries from same codebase:**
   ```
   proxy/
   â”œâ”€â”€ cmd/
   â”‚   â”œâ”€â”€ proxy-core/main.go      # New: minimal build
   â”‚   â””â”€â”€ proxy-data/main.go      # New: full build
   â””â”€â”€ internal/                    # Shared packages
   ```

2. **proxy-core includes:**
   - `handler.Messages()` - critical proxy endpoint
   - `handler.Models()`, `handler.Health()`
   - `provider/*` - all provider code
   - `service.ModelRouter`
   - `storage_sqlite.SaveRequest()` only (write path)
   - `storage_sqlite.UpdateRequestWithResponse()` only

3. **proxy-data includes:**
   - All `/api/*` handlers
   - `service.ConversationIndexer` (background goroutine)
   - Full `storage_sqlite` (read + indexing writes)
   - All stats/analytics code

4. **Shared packages:**
   - `internal/model` - data models (identical in both)
   - `internal/config` - configuration (filtered per service)
   - `internal/service/storage.go` - interface only

**File split breakdown:**

| File | proxy-core | proxy-data | Notes |
|------|-----------|-----------|-------|
| `cmd/proxy/main.go` | Split | Split | Two new mains |
| `handler/handlers.go` (lines 52-140) | âœ… | âŒ | Messages handler |
| `handler/handlers.go` (lines 192-1027) | âŒ | âœ… | Dashboard APIs |
| `handler/handlers_v2.go` | âŒ | âœ… | All V2 APIs |
| `provider/*.go` | âœ… | âŒ | Provider integration |
| `service/model_router.go` | âœ… | âŒ | Routing logic |
| `service/indexer.go` | âŒ | âœ… | Background indexing |
| `service/storage_sqlite.go` (SaveRequest) | âœ… | âœ… | Both write |
| `service/storage_sqlite.go` (GetStats) | âŒ | âœ… | Read-only APIs |
| `service/conversation.go` | âŒ | âœ… | Conversation parsing |
| `model/models.go` | âœ… | âœ… | Shared types |
| `config/config.go` | âœ… | âœ… | Shared config |

**SQLite concurrency setup:**
```go
// Both services use:
dbPath := cfg.Storage.DBPath + "?_journal_mode=WAL&_busy_timeout=5000"
```

Already configured! WAL mode allows:
- 1 writer + N readers concurrently
- proxy-core writes raw logs (fast INSERTs + UPDATEs)
- proxy-data reads for queries, writes for indexing (batch transactions)

**Deployment:**
```bash
# Start both services
./bin/proxy-core &    # Port 3001 - proxy traffic
./bin/proxy-data &    # Port 3002 - dashboard traffic

# Or use systemd/supervisor to manage
```

**Web dashboard changes:**
```typescript
// In web/app - point API calls to proxy-data
const API_BASE = 'http://localhost:3002'  // Was 3001
```

---

### Phase 2 (Future): Move to HTTP API or Queue

**Trigger conditions for Phase 2:**
- SQLite lock contention detected (>100ms waits)
- Need to run services on separate hosts
- Want true zero-downtime deploys (queue buffers during restart)

**Migration path:**
1. Add HTTP API to proxy-data: `POST /internal/log`
2. Make proxy-core call it asynchronously
3. Add circuit breaker (if proxy-data down, buffer in-memory or drop)
4. Remove SQLite writes from proxy-core
5. Now services fully independent

---

## Code Changes Required (Phase 1)

### 1. Create proxy-core/main.go

```go
package main

import (
    "log"
    "net/http"

    "github.com/gorilla/mux"
    "github.com/seifghazi/claude-code-monitor/internal/config"
    "github.com/seifghazi/claude-code-monitor/internal/handler"
    "github.com/seifghazi/claude-code-monitor/internal/service"
)

func main() {
    logger := log.New(os.Stdout, "proxy-core: ", log.LstdFlags)

    cfg, _ := config.Load()

    // Initialize providers
    providers := initProviders(cfg)
    modelRouter := service.NewModelRouter(cfg, providers, logger)

    // Storage for request logging ONLY
    storage, _ := service.NewSQLiteStorageService(&cfg.Storage)

    // Handler with minimal surface area
    h := handler.NewCoreHandler(storage, logger, modelRouter, cfg)

    r := mux.NewRouter()
    r.HandleFunc("/v1/messages", h.Messages).Methods("POST")
    r.HandleFunc("/v1/models", h.Models).Methods("GET")
    r.HandleFunc("/health", h.Health).Methods("GET")

    srv := &http.Server{
        Addr:    ":" + cfg.Server.Port,  // 3001
        Handler: r,
    }

    logger.Println("ðŸš€ proxy-core ready on", cfg.Server.Port)
    srv.ListenAndServe()
}
```

### 2. Create proxy-data/main.go

```go
package main

import (
    "log"
    "net/http"

    "github.com/gorilla/mux"
    "github.com/seifghazi/claude-code-monitor/internal/config"
    "github.com/seifghazi/claude-code-monitor/internal/handler"
    "github.com/seifghazi/claude-code-monitor/internal/service"
)

func main() {
    logger := log.New(os.Stdout, "proxy-data: ", log.LstdFlags)

    cfg, _ := config.Load()
    cfg.Server.Port = "3002"  // Different port

    // Full storage access
    storage, _ := service.NewSQLiteStorageService(&cfg.Storage)

    // Start conversation indexer
    indexer, _ := service.NewConversationIndexer(storage.(*service.SQLiteStorageService))
    indexer.Start()
    defer indexer.Stop()

    // Handler with data APIs
    h := handler.NewDataHandler(storage, logger, cfg)

    r := mux.NewRouter()

    // All dashboard APIs
    r.HandleFunc("/api/requests", h.GetRequests).Methods("GET")
    r.HandleFunc("/api/requests/summary", h.GetRequestsSummary).Methods("GET")
    r.HandleFunc("/api/requests/{id}", h.GetRequestByID).Methods("GET")
    r.HandleFunc("/api/stats", h.GetStats).Methods("GET")
    r.HandleFunc("/api/stats/hourly", h.GetHourlyStats).Methods("GET")
    // ... all other /api/* routes

    srv := &http.Server{
        Addr:    ":3002",
        Handler: r,
    }

    logger.Println("ðŸš€ proxy-data ready on 3002")
    srv.ListenAndServe()
}
```

### 3. Split Handler into CoreHandler and DataHandler

**handler/core_handler.go** (new):
```go
type CoreHandler struct {
    storage     service.StorageService  // Write-only usage
    modelRouter *service.ModelRouter
    logger      *log.Logger
    config      *config.Config
}

func (h *CoreHandler) Messages(w http.ResponseWriter, r *http.Request) {
    // Existing Messages implementation (lines 52-140)
    // Only uses:
    // - h.modelRouter.DetermineRoute()
    // - h.storage.SaveRequest()
    // - h.storage.UpdateRequestWithResponse()
}

func (h *CoreHandler) Models(w http.ResponseWriter, r *http.Request) {
    // Existing implementation
}

func (h *CoreHandler) Health(w http.ResponseWriter, r *http.Request) {
    // Existing implementation
}
```

**handler/data_handler.go** (new):
```go
type DataHandler struct {
    storage service.StorageService  // Full read/write
    logger  *log.Logger
    config  *config.Config
}

func (h *DataHandler) GetRequests(w http.ResponseWriter, r *http.Request) {
    // Existing implementation (line 192+)
}

func (h *DataHandler) GetRequestsSummary(...) {
    // Existing implementation
}

// ... all other dashboard handlers
```

---

## Migration Complexity Assessment

### Low Risk Areas âœ…

**These move cleanly:**
- Provider code â†’ proxy-core (zero dependencies on data layer)
- Indexer â†’ proxy-data (already background goroutine, can run anywhere)
- Stats APIs â†’ proxy-data (read-only, no proxy interaction)
- Model routing â†’ proxy-core (self-contained logic)

### Medium Risk Areas âš ï¸

**Need careful handling:**

1. **SQLite Write Conflicts**
   - Both services write to `requests` table
   - **Mitigation:** WAL mode (already enabled), use transactions
   - **Test:** Simulate concurrent writes (100 req/sec from proxy-core + indexer batch writes)

2. **Configuration Sharing**
   - Both need `storage.db_path`
   - Only proxy-core needs `providers.*`
   - **Mitigation:** Each service validates and loads only what it needs

3. **Shared Type Definitions**
   - Both need identical `model.RequestLog`, etc.
   - **Mitigation:** Keep `internal/model` as shared package (Go modules make this trivial)

### High Risk Areas ðŸ”´

**Potential blockers:**

1. **Dashboard Assumes Single Service**
   - Web app currently points to `localhost:3001` for ALL requests
   - **Impact:** Need to configure two base URLs or use reverse proxy
   - **Mitigation:**
     - Option 1: nginx/caddy in front, routes `/v1/*` â†’ 3001, `/api/*` â†’ 3002
     - Option 2: Update web app with two API clients

2. **Startup Dependencies**
   - What if proxy-data is down but proxy-core is up?
   - Currently, proxy happily logs requests even if indexer fails
   - **Impact:** Should be fine - proxy-core doesn't depend on proxy-data
   - **Verification needed:** Test proxy-core with proxy-data stopped

3. **Data Consistency During Split**
   - If migrating live system, need to ensure no data loss
   - **Impact:** Must drain requests before switching
   - **Mitigation:** Can run both old monolith + new split for overlap period

---

## Testing Strategy

### Before Split (Baseline)

1. **Current performance:**
   ```bash
   # Measure current throughput
   wrk -t4 -c100 -d30s --latency http://localhost:3001/v1/messages
   ```

2. **Current DB metrics:**
   ```bash
   # Check SQLite lock waits
   sqlite3 requests.db "PRAGMA wal_checkpoint(TRUNCATE);"
   # Measure write latency
   ```

### After Split (Validation)

1. **proxy-core isolation:**
   ```bash
   # Kill proxy-data
   pkill proxy-data

   # Verify proxy-core still proxies requests
   curl -X POST http://localhost:3001/v1/messages
   # Should work! Core is independent
   ```

2. **proxy-data isolation:**
   ```bash
   # Kill proxy-core
   pkill proxy-core

   # Verify dashboard still serves data
   curl http://localhost:3002/api/stats
   # Should work! Data service serves existing data
   ```

3. **Concurrent write test:**
   ```bash
   # Hammer proxy-core with requests (writes to requests table)
   wrk -t4 -c50 -d60s http://localhost:3001/v1/messages &

   # Simultaneously trigger indexer (writes to conversations/FTS tables)
   # Watch for SQLite lock errors in logs
   ```

4. **End-to-end flow:**
   ```bash
   # Send request via proxy-core
   curl -X POST http://localhost:3001/v1/messages -d @test.json

   # Wait 2 seconds
   sleep 2

   # Verify it appears in proxy-data stats
   curl http://localhost:3002/api/stats | jq '.todayRequests'
   # Should increment
   ```

---

## Open Questions for User

### 1. Deployment Preferences

**Question:** How do you plan to deploy these services?

**Options:**
- **Single host:** Both on localhost (3001/3002) - simplest
- **Reverse proxy:** nginx/caddy routes by path - cleanest for web app
- **Separate hosts:** Different machines - need queue/HTTP (Phase 2)

**Recommendation:** Start with single host + reverse proxy. This allows web app to keep hitting one URL.

**Example nginx config:**
```nginx
server {
    listen 3000;

    location /v1/ {
        proxy_pass http://localhost:3001;  # proxy-core
    }

    location /api/ {
        proxy_pass http://localhost:3002;  # proxy-data
    }
}
```

### 2. Data Consistency Requirements

**Question:** Is it acceptable for dashboard stats to lag ~1-5 seconds behind live requests?

**Context:** With Option A (shared DB), stats are immediately available. With Option B/C (queue/HTTP), there's slight delay.

**Implication:** If you need real-time stats, stick with Option A. If eventual consistency is fine, Option C is cleaner.

### 3. Future Scale Plans

**Question:** Do you ever plan to:
- Run on multiple machines?
- Handle >1000 requests/second?
- Deploy to cloud (containers, k8s)?

**Why it matters:** If yes to any, invest in Option C (HTTP API) or Option B (queue) now. If no, Option A is simpler and sufficient.

### 4. Downtime Tolerance

**Question:** Can proxy-core tolerate proxy-data being down for a few seconds?

**Current behavior:** Proxy keeps working, just doesn't index conversations or serve stats.

**Is this acceptable?** If yes, services are truly independent. If no, need to add health checks and failover logic.

---

## Risks and Dependencies

### Critical Dependencies

**Must Have:**
1. SQLite with WAL mode (already configured âœ…)
2. Separate ports available (3001, 3002)
3. Shared file access if on same host (filesystem, Docker volumes)

**Nice to Have:**
1. Reverse proxy (nginx/caddy) for unified web endpoint
2. Process manager (systemd, supervisor) for service lifecycle
3. Monitoring (Prometheus/Grafana) to detect split issues

### Known Risks

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| SQLite write conflicts | Low | Medium | WAL mode + testing |
| Config drift (services diverge) | Medium | High | Shared `internal/` packages |
| Web app breaks (wrong URLs) | High | High | Add reverse proxy OR update web config |
| Indexer stops working | Low | Low | Already isolated, just needs DB access |
| proxy-core becomes stateless but loses logs | Low | Medium | Ensure SaveRequest() works before removing |

### Ambiguities Needing Clarification

**NEEDS_CLARIFICATION:**

1. **Web App API Base URL**
   - Current: Single `http://localhost:3001` for everything
   - **After split:** Need two URLs or reverse proxy
   - **Question:** Do you prefer:
     - A) Keep web app simple, add nginx reverse proxy
     - B) Update web app to call two different ports
   - **Impact:** If B, need to update every API call in React app

2. **Conversation Indexer Failure Mode**
   - **Current behavior:** If indexer fails to start, proxy logs warning and continues
   - **After split:** Should proxy-data refuse to start if indexer fails?
   - **Question:** Is conversation search critical, or nice-to-have?
   - **Impact:** Affects error handling in proxy-data startup

3. **Database File Location**
   - **Current:** Hardcoded `requests.db` in working directory
   - **After split:** Both services need same path
   - **Question:** Use absolute path in config? Or rely on both running in same directory?
   - **Impact:** Docker deployments need shared volume

4. **Graceful Shutdown**
   - **Current:** Both indexer and proxy shut down together
   - **After split:** What if proxy-data is restarting (indexing) when proxy-core sends traffic?
   - **Question:** Is it OK for stats to be briefly stale during proxy-data restarts?
   - **Impact:** Need buffering or "indexer busy" state if not acceptable

---

## Recommendations

### Immediate Next Steps (Phase 1)

1. **Prototype the split** (1-2 hours)
   - Create `cmd/proxy-core/main.go` (copy main.go, strip data handlers)
   - Create `cmd/proxy-data/main.go` (copy main.go, strip proxy handlers)
   - Build both: `go build ./cmd/proxy-core` and `go build ./cmd/proxy-data`
   - Run side-by-side, verify isolation

2. **Test concurrent access** (30 mins)
   - Run both services
   - Send requests through proxy-core
   - Query stats from proxy-data
   - Check logs for SQLite errors

3. **Add reverse proxy** (15 mins)
   ```nginx
   location /v1/ { proxy_pass http://localhost:3001; }
   location /api/ { proxy_pass http://localhost:3002; }
   ```

4. **Update web app config** (5 mins)
   ```typescript
   const PROXY_URL = 'http://localhost:3000'  // nginx
   ```

**Total effort: ~3 hours for working prototype**

### Future Work (Phase 2+)

**Only if needed:**
- Move to HTTP API between services (if scaling to multiple hosts)
- Add message queue (if >1000 req/sec or need buffer)
- Split web dashboard into separate service (currently coupled to proxy via Remix)

---

## Conclusion

**The split is feasible and beneficial:**

âœ… **Natural boundaries exist** - code cleanly separates into proxy vs. data concerns
âœ… **Low migration risk** - shared SQLite database handles concurrency well
âœ… **Clear deployment path** - can start simple (shared DB) and evolve (HTTP/queue)
âœ… **Achieves goal** - proxy-core becomes stable, data service can restart freely

**Recommended approach:**

**Phase 1 (NOW):** Shared database split
- Two binaries, same codebase, shared SQLite
- Add reverse proxy for unified web endpoint
- Test with realistic load

**Phase 2 (LATER, if needed):** HTTP API decoupling
- Only if scaling to multiple hosts
- Or if SQLite contention detected
- Or if want true zero-downtime (queue buffering)

**The biggest unknown:** Web app API configuration. Need to decide on reverse proxy vs. dual API clients.

**Ready to proceed?** The code is well-structured for this split. Main work is testing and handling the web app API routing.
