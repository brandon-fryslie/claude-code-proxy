# Definition of Done: Service Split Implementation

**Generated:** 2025-12-28-152348
**Plan:** PLAN-2025-12-28-152348.md
**Sprint Goal:** Split monolithic proxy into proxy-core and proxy-data services

---

## Sprint Scope

**This sprint delivers:**
1. proxy-core binary (lightweight proxy)
2. proxy-data binary (data processing + dashboard APIs)
3. Caddy reverse proxy for unified routing

**Deferred:**
- HTTP API between services (Phase 2)
- Message queue buffering (Phase 2)
- Docker Compose setup
- Advanced monitoring/metrics

---

## Acceptance Criteria

### P0-1: Create proxy-core Binary

- [ ] New binary `cmd/proxy-core/main.go` created that compiles successfully
- [ ] Binary includes ONLY /v1/messages, /v1/models, /health endpoints (no /api/* routes)
- [ ] Provider integration code (anthropic.go, openai.go) fully integrated
- [ ] ModelRouter for subagent detection and routing included
- [ ] Storage service limited to SaveRequest() and UpdateRequestWithResponse() - no read paths, no stats, no indexing
- [ ] Streaming response handling (SSE parsing) works identically to monolith
- [ ] Config loading filters to only provider + storage sections needed
- [ ] Server starts on configurable port (default 3001)
- [ ] Unit tests pass for all included handlers
- [ ] Integration test: Can proxy a request to Anthropic API and log to SQLite

### P0-2: Create proxy-data Binary

- [ ] New binary `cmd/proxy-data/main.go` created that compiles successfully
- [ ] Binary includes ALL /api/* and /api/v2/* endpoints (no /v1/* routes except possibly /health)
- [ ] Full storage service with all read methods (GetStats, GetRequests, etc.)
- [ ] Conversation indexer starts as background goroutine on service startup
- [ ] Indexer fail-fast on error: if indexer.Start() returns error, service refuses to start
- [ ] All stats endpoints work: /api/stats, /api/stats/hourly, /api/stats/models
- [ ] All request endpoints work: /api/requests, /api/requests/summary, /api/requests/{id}
- [ ] All conversation endpoints work: /api/conversations, /api/conversations/search
- [ ] Server starts on configurable port (default 3002)
- [ ] Unit tests pass for all data handlers
- [ ] Integration test: Can query stats and search conversations after proxy-core writes data

### P0-3: Split Handler Code into CoreHandler and DataHandler

- [ ] New file `handler/core_handler.go` created with CoreHandler struct and methods
- [ ] CoreHandler includes: Messages(), Models(), Health()
- [ ] CoreHandler has minimal dependencies: StorageService (write-only), ModelRouter, Logger, Config
- [ ] New file `handler/data_handler.go` created with DataHandler struct and methods
- [ ] DataHandler includes: GetRequests(), GetRequestsSummary(), GetRequestByID(), GetStats(), GetHourlyStats(), GetModelStats(), GetConversations(), SearchConversations()
- [ ] DataHandler has full dependencies: StorageService (read/write), Logger, Config
- [ ] All V2 handlers either integrated into DataHandler or kept in handlers_v2.go (both are acceptable)
- [ ] Original `handlers.go` can be deleted or kept as compatibility wrapper (preference: delete to avoid confusion)
- [ ] All existing unit tests updated to use new handler types
- [ ] No functional changes - behavior identical to before split

### P1-1: Add Reverse Proxy (Caddy) for Unified Routing

- [ ] Caddyfile created at project root with reverse proxy configuration
- [ ] Caddy routes `/v1/messages`, `/v1/models`, `/v1/health` to localhost:3001
- [ ] Caddy routes all `/api/*` paths to localhost:3002
- [ ] Caddy listens on port 3000 (configurable)
- [ ] Health check endpoint works: `curl http://localhost:3000/health` returns 200
- [ ] Proxy endpoint works: `curl -X POST http://localhost:3000/v1/messages` reaches proxy-core
- [ ] Dashboard API works: `curl http://localhost:3000/api/stats` reaches proxy-data
- [ ] Caddy handles streaming responses correctly (SSE from /v1/messages)
- [ ] CORS headers preserved from backend services
- [ ] README.md updated with Caddy setup instructions

### P1-2: Update Configuration for Absolute Database Path

- [ ] config.yaml updated with absolute database path (e.g., `/data/requests.db` or `${DATA_DIR}/requests.db`)
- [ ] Environment variable expansion supported: `${DATA_DIR}` resolves to env var or defaults to `./data`
- [ ] Both proxy-core and proxy-data read same config and connect to same database
- [ ] config.yaml.example updated with absolute path examples
- [ ] Documentation explains: "Both services must have access to same database file - use absolute path"
- [ ] Makefile updated: `make db-reset` uses new absolute path
- [ ] Migration tested: Starting both services with new config.yaml successfully shares database
- [ ] Error message if database path not found: "Database path ${path} does not exist. Create directory first."

### P1-3: Update Build and Run Scripts

- [ ] Makefile includes `build-proxy-core` target: `cd proxy && go build -o ../bin/proxy-core ./cmd/proxy-core`
- [ ] Makefile includes `build-proxy-data` target: `cd proxy && go build -o ../bin/proxy-data ./cmd/proxy-data`
- [ ] Makefile `build-proxy` target builds BOTH binaries (replaces old single binary)
- [ ] run.sh updated to start: Caddy (port 3000) + proxy-core (port 3001) + proxy-data (port 3002)
- [ ] run.sh properly backgrounds services and captures PIDs for graceful shutdown
- [ ] justfile updated with commands: `just build-core`, `just build-data`, `just run-split`
- [ ] `make dev` now runs the split architecture (Caddy + both services + web dashboard)
- [ ] `make clean` removes both binaries: `rm -f bin/proxy-core bin/proxy-data`
- [ ] README.md updated with new build/run instructions
- [ ] Shutdown script (Ctrl+C) gracefully stops all three services

### P2-1: Add Service Health Checks and Monitoring

- [ ] proxy-core /health endpoint returns JSON with: {status: "ok", database: "connected", providers: ["anthropic", "openai"]}
- [ ] proxy-data /health endpoint returns JSON with: {status: "ok", database: "connected", indexer: "running"}
- [ ] Health check returns 503 (Service Unavailable) if database connection fails
- [ ] proxy-data health check returns 503 if indexer has crashed (aligns with fail-fast requirement)
- [ ] Optional: /metrics endpoint exposing Prometheus metrics (request count, latency)
- [ ] Documentation explains: "Use /health for load balancer health checks"

### P2-2: Verify Web Dashboard Works with Split Services

- [ ] Web dashboard starts successfully: `cd web && npm run dev`
- [ ] Dashboard fetches stats correctly from /api/stats (proxied to proxy-data)
- [ ] Request list loads correctly from /api/requests/summary
- [ ] Request detail view loads correctly from /api/requests/{id}
- [ ] Conversation search works correctly from /api/conversations
- [ ] No CORS errors in browser console
- [ ] No broken API calls (all 2xx responses)
- [ ] Performance is identical to monolith (no added latency from Caddy)
- [ ] Manual test: Send request via Claude Code → appears in dashboard within 2 seconds

### P3-1: Add Integration Tests for Service Split

- [ ] Test: proxy-core can proxy requests while proxy-data is stopped (independence)
- [ ] Test: proxy-data can serve stats while proxy-core is stopped (independence)
- [ ] Test: Concurrent writes - proxy-core saves 100 requests while indexer processes conversations (no SQLite lock errors)
- [ ] Test: End-to-end flow - request via proxy-core appears in proxy-data stats within 2 seconds
- [ ] Test: Indexer fail-fast - proxy-data refuses to start if indexer initialization fails
- [ ] Test: Database path resolution - both services connect to same database with absolute path
- [ ] All tests pass in CI/CD pipeline
- [ ] Test documentation explains how to run: `go test ./integration/...`

---

## Success Criteria (End-to-End Verification)

**Functional Requirements:**
- ✅ Can send Claude Code request through proxy-core → request logged to SQLite
- ✅ Can view request in web dashboard via proxy-data → data appears correctly
- ✅ Can restart proxy-data without dropping requests (proxy-core continues working)
- ✅ Can restart proxy-core without breaking dashboard (proxy-data continues serving)
- ✅ Conversation indexer processes new conversations within 2 seconds
- ✅ FTS5 search returns correct results from indexed conversations

**Non-Functional Requirements:**
- ✅ No SQLite lock errors under concurrent load (100 req/sec test)
- ✅ Latency identical to monolith (within 5ms)
- ✅ All existing unit tests pass (no regressions)
- ✅ Build time < 10 seconds for both binaries
- ✅ Memory usage per service < monolith memory usage (should be lower due to smaller binary size)

**Operational Requirements:**
- ✅ `make dev` starts all services with one command
- ✅ `make build-proxy` builds both binaries successfully
- ✅ Logs clearly distinguish between services (prefixes: "proxy-core:", "proxy-data:")
- ✅ Health checks accurately reflect service status
- ✅ README.md has clear instructions for running split services

**Quality Gates:**
- ✅ Go tests: `cd proxy && go test ./...` - all pass
- ✅ Go build: `cd proxy && go build ./cmd/proxy-core && go build ./cmd/proxy-data` - no errors
- ✅ Web tests: `cd web && npm run test` - all pass
- ✅ Type check: `cd web && npm run typecheck` - no errors
- ✅ Lint: `cd web && npm run lint` - no errors

---

## Manual Testing Checklist

**Before declaring sprint complete, manually verify:**

1. **Service Isolation:**
   - [ ] Start only proxy-core → health check returns 200
   - [ ] Send test request → request proxied successfully
   - [ ] Stop proxy-core, start only proxy-data → health check returns 200
   - [ ] Query stats → data returned successfully

2. **Reverse Proxy Routing:**
   - [ ] Start Caddy + both services
   - [ ] `curl http://localhost:3000/health` → returns proxy-core health
   - [ ] `curl -X POST http://localhost:3000/v1/messages` → proxies to Anthropic
   - [ ] `curl http://localhost:3000/api/stats` → returns stats from proxy-data

3. **Web Dashboard:**
   - [ ] Open http://localhost:5173 (Vite dev server)
   - [ ] Request list loads and displays data
   - [ ] Click request → detail view loads
   - [ ] Search conversations → results appear
   - [ ] Check Network tab → all requests to localhost:3000, no errors

4. **Database Sharing:**
   - [ ] Send request via proxy-core
   - [ ] Check SQLite: `sqlite3 /data/requests.db "SELECT COUNT(*) FROM requests;"`
   - [ ] Query stats via proxy-data → includes new request
   - [ ] Wait 2 seconds → conversation indexed

5. **Graceful Shutdown:**
   - [ ] Start all services via `make dev`
   - [ ] Press Ctrl+C
   - [ ] Verify all services stop cleanly (no orphan processes)

---

## Definition of "Done"

This sprint is **DONE** when:

1. ✅ All P0 and P1 acceptance criteria are met (checkboxes checked)
2. ✅ Manual testing checklist completed with no failures
3. ✅ All quality gates pass (tests, build, lint, typecheck)
4. ✅ README.md updated with split architecture documentation
5. ✅ Can run `make dev` and have fully working system (Caddy + proxy-core + proxy-data + web)
6. ✅ Can send Claude Code request → appears in web dashboard
7. ✅ No regressions (all features work identically to monolith)

**Optional (P2/P3 items can be deferred to future sprints):**
- P2-1: Enhanced health checks
- P2-2: Comprehensive web dashboard verification
- P3-1: Integration test suite

---

## Rollback Criteria

**Abort and rollback to monolith if:**

- ❌ SQLite lock errors occur under normal load (< 100 req/sec)
- ❌ Latency increases by > 20ms compared to monolith
- ❌ Web dashboard features break (e.g., stats don't update, search fails)
- ❌ Existing unit tests fail and cannot be fixed within 1 day
- ❌ Services crash or hang under normal operation

**These issues indicate the split is not ready for production.**

---

## Next Steps After Sprint Complete

Once this sprint is DONE:

1. **Monitor production metrics** for 1 week
   - Track SQLite lock contention
   - Measure service restart times
   - Verify stats lag is acceptable

2. **Plan Phase 2** (if needed)
   - Evaluate need for HTTP API between services
   - Consider message queue for buffering
   - Plan Docker Compose deployment

3. **Document lessons learned**
   - Update CLAUDE.md with split architecture details
   - Document any gotchas or edge cases discovered
   - Update troubleshooting guide

4. **Future enhancements**
   - Prometheus metrics integration
   - Grafana dashboards per service
   - Multi-host deployment support
   - Kubernetes manifests
