# Implementation Plan: Service Split for Zero-Downtime Architecture

**Generated:** 2025-12-28-152348
**Planner:** status-planner
**Source STATUS:** STATUS-2025-12-28-144345.md
**Sprint Goal:** Split monolithic proxy into proxy-core and proxy-data services with shared database (Phase 1)

---

## Executive Summary

**Current State:** Single monolithic Go service (~7,000 LOC) handles both critical proxy operations (forwarding Claude API requests) and data processing/serving (indexing, stats, dashboard APIs). From STATUS-2025-12-28-144345.md: "Both services need access" to the SQLite database, creating coupling.

**Target State:** Two independent services:
1. **proxy-core** - Lightweight proxy (rarely changes), handles /v1/messages, /v1/models, /health
2. **proxy-data** - Data processing (frequent changes), handles /api/*, indexing, stats

**Total Gap:** 9 work items (3 P0, 3 P1, 2 P2, 1 P3)

**Sprint Scope:** Deliver P0 + P1 items (proxy-core binary, proxy-data binary, reverse proxy routing) - approximately 2-3 deliverables as specified.

**Recommended Focus:** Start with P0 items to establish the foundation, then proceed to P1 for completeness, defer P2/P3 optimizations.

---

## Backlog by Priority

### P0 (Critical): Foundation Components

---

## P0-1: Create proxy-core Binary

**Status:** Not Started
**Complexity:** Medium
**Dependencies:** None
**Spec Reference:** CLAUDE.md (Proxy Server Architecture) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md sections "Code Distribution (Critical Path)", "Proposed Architecture (Option A)"

### Description

Create a new `cmd/proxy-core/main.go` that compiles a minimal proxy service containing ONLY the critical request forwarding path. This binary must be lightweight and stable, handling only /v1/messages, /v1/models, and /health endpoints. All data processing, indexing, and dashboard APIs are excluded.

From the STATUS file: "proxy-core territory" includes:
- `/v1/messages` handler (handlers.go:52-140)
- Provider routing (model_router.go:169-230)
- Provider implementations (provider/*.go)
- Request/response streaming (handlers.go:521-769)
- Basic request logging (storage_sqlite.go:202-246) - SaveRequest() and UpdateRequestWithResponse() ONLY

### Acceptance Criteria

- [ ] New binary `cmd/proxy-core/main.go` created that compiles successfully
- [ ] Binary includes ONLY /v1/messages, /v1/models, /health endpoints (no /api/* routes)
- [ ] Provider integration code (anthropic.go, openai.go) fully integrated
- [ ] ModelRouter for subagent detection and routing included
- [ ] Storage service limited to SaveRequest() and UpdateRequestWithResponse() - no read paths, no stats, no indexing
- [ ] Streaming response handling (SSE parsing) works identically to monolith
- [ ] Config loading filters to only provider + storage sections needed
- [ ] Server starts on configurable port (default 3001)
- [ ] Unit tests pass for all included handlers
- [ ] Integration test: Can proxy a request to Anthropic API and log to SQLite

### Technical Notes

**Key files to include:**
- `cmd/proxy-core/main.go` - new entry point
- `handler/handlers.go` (lines 52-140 only - Messages handler)
- `handler/utils.go` (if used by Messages)
- `provider/*` - all provider code
- `service/model_router.go` - routing logic
- `service/storage.go` - interface
- `service/storage_sqlite.go` - ONLY SaveRequest() and UpdateRequestWithResponse() methods
- `model/models.go` - shared types
- `config/config.go` - full config (filter at runtime)
- `middleware/logging.go` - request logging

**Architecture pattern:**
```go
type CoreHandler struct {
    storage     service.StorageService  // Write-only usage
    modelRouter *service.ModelRouter
    logger      *log.Logger
    config      *config.Config
}
```

**Critical:** Ensure SQLite connection uses WAL mode (already configured in storage_sqlite.go):
```go
dbPath := cfg.Storage.DBPath + "?_journal_mode=WAL&_busy_timeout=5000"
```

**Build command:**
```bash
cd proxy && go build -o ../bin/proxy-core ./cmd/proxy-core
```

---

## P0-2: Create proxy-data Binary

**Status:** Not Started
**Complexity:** Medium
**Dependencies:** None (can build in parallel with P0-1)
**Spec Reference:** CLAUDE.md (Dashboard API, Conversation Indexer) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md sections "Code Distribution (Data Processing)", "Proposed Architecture (Option A)"

### Description

Create a new `cmd/proxy-data/main.go` that compiles a data service containing ALL dashboard APIs, stats aggregation, conversation indexing, and FTS5 search. This service has full read/write access to the SQLite database and runs the background conversation indexer goroutine.

From the STATUS file: "proxy-data territory" includes:
- Conversation indexer (indexer.go:1-357) - background goroutine
- FTS5 search (storage_sqlite.go:1337-1440)
- Statistics aggregation (storage_sqlite.go:742-1335)
- Dashboard APIs (handlers.go:192-1027, handlers_v2.go)

### Acceptance Criteria

- [ ] New binary `cmd/proxy-data/main.go` created that compiles successfully
- [ ] Binary includes ALL /api/* and /api/v2/* endpoints (no /v1/* routes except possibly /health)
- [ ] Full storage service with all read methods (GetStats, GetRequests, etc.)
- [ ] Conversation indexer starts as background goroutine on service startup
- [ ] Indexer fail-fast on error: if indexer.Start() returns error, service refuses to start (user decision: "Fail startup if indexer fails")
- [ ] All stats endpoints work: /api/stats, /api/stats/hourly, /api/stats/models
- [ ] All request endpoints work: /api/requests, /api/requests/summary, /api/requests/{id}
- [ ] All conversation endpoints work: /api/conversations, /api/conversations/search
- [ ] Server starts on configurable port (default 3002)
- [ ] Unit tests pass for all data handlers
- [ ] Integration test: Can query stats and search conversations after proxy-core writes data

### Technical Notes

**Key files to include:**
- `cmd/proxy-data/main.go` - new entry point
- `handler/handlers.go` (lines 192-1027 - all dashboard APIs)
- `handler/handlers_v2.go` - all V2 APIs
- `service/indexer.go` - conversation indexer
- `service/conversation.go` - conversation parsing
- `service/storage_sqlite.go` - FULL implementation (all methods)
- `model/models.go` - shared types
- `config/config.go` - full config (filter at runtime)

**Architecture pattern:**
```go
type DataHandler struct {
    storage service.StorageService  // Full read/write
    logger  *log.Logger
    config  *config.Config
}
```

**Startup sequence:**
```go
func main() {
    // ... init config, storage ...

    // Start indexer - FAIL FAST
    indexer, err := service.NewConversationIndexer(storage.(*service.SQLiteStorageService))
    if err != nil {
        logger.Fatalf("Failed to create indexer: %v", err)
    }
    if err := indexer.Start(); err != nil {
        logger.Fatalf("Failed to start indexer: %v", err)
    }
    defer indexer.Stop()

    // ... setup handlers and HTTP server ...
}
```

**Build command:**
```bash
cd proxy && go build -o ../bin/proxy-data ./cmd/proxy-data
```

---

## P0-3: Split Handler Code into CoreHandler and DataHandler

**Status:** Not Started
**Complexity:** Small
**Dependencies:** None (supports P0-1 and P0-2)
**Spec Reference:** CLAUDE.md (Handler Architecture) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md section "Code Changes Required (Split Handler)"

### Description

Extract handler logic into two separate handler types: `CoreHandler` (minimal proxy endpoints) and `DataHandler` (all dashboard APIs). This separation ensures each binary only includes the handler methods it needs, reducing surface area and making the split explicit in code structure.

Currently `handler/handlers.go` is a monolith with both proxy and data methods. Split this into:
- `handler/core_handler.go` - Messages, Models, Health (lines 52-140)
- `handler/data_handler.go` - All dashboard APIs (lines 192-1027)
- `handler/handlers_v2.go` - Already separate, move to data_handler.go or keep as-is

### Acceptance Criteria

- [ ] New file `handler/core_handler.go` created with CoreHandler struct and methods
- [ ] CoreHandler includes: Messages(), Models(), Health()
- [ ] CoreHandler has minimal dependencies: StorageService (write-only), ModelRouter, Logger, Config
- [ ] New file `handler/data_handler.go` created with DataHandler struct and methods
- [ ] DataHandler includes: GetRequests(), GetRequestsSummary(), GetRequestByID(), GetStats(), GetHourlyStats(), GetModelStats(), GetConversations(), SearchConversations()
- [ ] DataHandler has full dependencies: StorageService (read/write), Logger, Config
- [ ] All V2 handlers either integrated into DataHandler or kept in handlers_v2.go (both are acceptable)
- [ ] Original `handlers.go` can be deleted or kept as compatibility wrapper (preference: delete to avoid confusion)
- [ ] All existing unit tests updated to use new handler types
- [ ] No functional changes - behavior identical to before split

### Technical Notes

**CoreHandler signature:**
```go
type CoreHandler struct {
    storage     service.StorageService  // Write-only usage
    modelRouter *service.ModelRouter
    logger      *log.Logger
    config      *config.Config
}

func NewCoreHandler(storage service.StorageService, logger *log.Logger, router *service.ModelRouter, cfg *config.Config) *CoreHandler
func (h *CoreHandler) Messages(w http.ResponseWriter, r *http.Request)
func (h *CoreHandler) Models(w http.ResponseWriter, r *http.Request)
func (h *CoreHandler) Health(w http.ResponseWriter, r *http.Request)
```

**DataHandler signature:**
```go
type DataHandler struct {
    storage service.StorageService  // Full read/write
    logger  *log.Logger
    config  *config.Config
}

func NewDataHandler(storage service.StorageService, logger *log.Logger, cfg *config.Config) *DataHandler
func (h *DataHandler) GetRequests(w http.ResponseWriter, r *http.Request)
// ... all other dashboard methods
```

**Migration path:**
1. Create core_handler.go with CoreHandler struct
2. Copy Messages, Models, Health from handlers.go
3. Create data_handler.go with DataHandler struct
4. Copy all dashboard methods from handlers.go
5. Update main.go references (if testing before full split)
6. Delete handlers.go or rename to handlers_deprecated.go
7. Update tests to import new files

---

### P1 (High): Core Features for MVP Completeness

---

## P1-1: Add Reverse Proxy (Caddy) for Unified Routing

**Status:** Not Started
**Complexity:** Small
**Dependencies:** P0-1, P0-2 (needs both services running)
**Spec Reference:** CLAUDE.md (API Endpoints) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md sections "Open Questions (Deployment Preferences)", "Recommended Split Strategy"

### Description

Add a Caddy reverse proxy that routes requests to the appropriate backend service based on URL path. This allows the web dashboard to continue using a single base URL (localhost:3000) instead of needing to know about two separate ports.

User decision (from context): "API Routing: Reverse proxy (nginx/caddy) - web apps unchanged"

From STATUS file: "Recommended approach: Add reverse proxy for unified web endpoint"

Routes:
- `/v1/*` ‚Üí proxy-core (localhost:3001)
- `/api/*` ‚Üí proxy-data (localhost:3002)

### Acceptance Criteria

- [ ] Caddyfile created at project root with reverse proxy configuration
- [ ] Caddy routes `/v1/messages`, `/v1/models`, `/v1/health` to localhost:3001
- [ ] Caddy routes all `/api/*` paths to localhost:3002
- [ ] Caddy listens on port 3000 (configurable)
- [ ] Health check endpoint works: `curl http://localhost:3000/health` returns 200
- [ ] Proxy endpoint works: `curl -X POST http://localhost:3000/v1/messages` reaches proxy-core
- [ ] Dashboard API works: `curl http://localhost:3000/api/stats` reaches proxy-data
- [ ] Caddy handles streaming responses correctly (SSE from /v1/messages)
- [ ] CORS headers preserved from backend services
- [ ] README.md updated with Caddy setup instructions

### Technical Notes

**Example Caddyfile:**
```
:3000 {
    # Proxy endpoints ‚Üí proxy-core
    reverse_proxy /v1/* localhost:3001

    # Dashboard APIs ‚Üí proxy-data
    reverse_proxy /api/* localhost:3002

    # Enable access logs
    log {
        output file /var/log/caddy/access.log
    }
}
```

**Installation (macOS):**
```bash
brew install caddy
```

**Running:**
```bash
caddy run --config Caddyfile
```

**Alternative: nginx** (if user prefers):
```nginx
server {
    listen 3000;

    location /v1/ {
        proxy_pass http://localhost:3001;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }

    location /api/ {
        proxy_pass http://localhost:3002;
    }
}
```

---

## P1-2: Update Configuration for Absolute Database Path

**Status:** Not Started
**Complexity:** Small
**Dependencies:** P0-1, P0-2 (needs both services)
**Spec Reference:** CLAUDE.md (Configuration) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md section "Open Questions (Database File Location)"

### Description

Update config.yaml to use an absolute database path instead of relative path. This ensures both proxy-core and proxy-data access the same SQLite database file regardless of working directory, and makes Docker deployment clearer.

User decision (from context): "Database Path: Absolute path in config - clear for Docker/remote"

From STATUS file: "Question: Use absolute path in config? Or rely on both running in same directory? Impact: Docker deployments need shared volume"

### Acceptance Criteria

- [ ] config.yaml updated with absolute database path (e.g., `/data/requests.db` or `${DATA_DIR}/requests.db`)
- [ ] Environment variable expansion supported: `${DATA_DIR}` resolves to env var or defaults to `./data`
- [ ] Both proxy-core and proxy-data read same config and connect to same database
- [ ] config.yaml.example updated with absolute path examples
- [ ] Documentation explains: "Both services must have access to same database file - use absolute path"
- [ ] Makefile updated: `make db-reset` uses new absolute path
- [ ] Migration tested: Starting both services with new config.yaml successfully shares database
- [ ] Error message if database path not found: "Database path ${path} does not exist. Create directory first."

### Technical Notes

**Config structure:**
```yaml
storage:
  db_path: "${DATA_DIR}/requests.db"  # Expands to env var or defaults to ./data/requests.db
```

**Config loading (config.go):**
```go
func Load() (*Config, error) {
    // ... existing loading ...

    // Expand environment variables in db_path
    cfg.Storage.DBPath = os.ExpandEnv(cfg.Storage.DBPath)

    // Convert to absolute if relative
    if !filepath.IsAbs(cfg.Storage.DBPath) {
        absPath, err := filepath.Abs(cfg.Storage.DBPath)
        if err != nil {
            return nil, fmt.Errorf("failed to resolve database path: %w", err)
        }
        cfg.Storage.DBPath = absPath
    }

    return cfg, nil
}
```

**Docker Compose example (future reference):**
```yaml
services:
  proxy-core:
    environment:
      DATA_DIR: /data
    volumes:
      - db-data:/data

  proxy-data:
    environment:
      DATA_DIR: /data
    volumes:
      - db-data:/data

volumes:
  db-data:
```

---

## P1-3: Update Build and Run Scripts

**Status:** Not Started
**Complexity:** Small
**Dependencies:** P0-1, P0-2, P1-1 (needs all components)
**Spec Reference:** CLAUDE.md (Build Commands) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md section "Deployment"

### Description

Update Makefile, run.sh, and justfile to support building and running the split services. Add targets for building both binaries, running them together, and managing their lifecycle.

### Acceptance Criteria

- [ ] Makefile includes `build-proxy-core` target: `cd proxy && go build -o ../bin/proxy-core ./cmd/proxy-core`
- [ ] Makefile includes `build-proxy-data` target: `cd proxy && go build -o ../bin/proxy-data ./cmd/proxy-data`
- [ ] Makefile `build-proxy` target builds BOTH binaries (replaces old single binary)
- [ ] run.sh updated to start: Caddy (port 3000) + proxy-core (port 3001) + proxy-data (port 3002)
- [ ] run.sh properly backgrounds services and captures PIDs for graceful shutdown
- [ ] justfile updated with commands: `just build-core`, `just build-data`, `just run-split`
- [ ] `make dev` now runs the split architecture (Caddy + both services + web dashboard)
- [ ] `make clean` removes both binaries: `rm -f bin/proxy-core bin/proxy-data`
- [ ] README.md updated with new build/run instructions
- [ ] Shutdown script (Ctrl+C) gracefully stops all three services

### Technical Notes

**Makefile additions:**
```makefile
build-proxy-core:
	cd proxy && go build -o ../bin/proxy-core ./cmd/proxy-core

build-proxy-data:
	cd proxy && go build -o ../bin/proxy-data ./cmd/proxy-data

build-proxy: build-proxy-core build-proxy-data

run-split: build-proxy
	./run-split.sh
```

**run-split.sh example:**
```bash
#!/bin/bash
set -e

# Start services
echo "Starting proxy-core on :3001..."
./bin/proxy-core &
PROXY_CORE_PID=$!

echo "Starting proxy-data on :3002..."
./bin/proxy-data &
PROXY_DATA_PID=$!

echo "Starting caddy on :3000..."
caddy run --config Caddyfile &
CADDY_PID=$!

# Trap exit signals
trap "kill $PROXY_CORE_PID $PROXY_DATA_PID $CADDY_PID" EXIT

# Wait for all services
wait
```

**Justfile additions:**
```just
build-core:
    cd proxy && go build -o ../bin/proxy-core ./cmd/proxy-core

build-data:
    cd proxy && go build -o ../bin/proxy-data ./cmd/proxy-data

run-split: build-core build-data
    ./run-split.sh
```

---

### P2 (Medium): Important Enhancements

---

## P2-1: Add Service Health Checks and Monitoring

**Status:** Not Started
**Complexity:** Medium
**Dependencies:** P0-1, P0-2 (needs both services)
**Spec Reference:** CLAUDE.md (Health endpoint) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md section "Open Questions (Downtime Tolerance)"

### Description

Enhance the /health endpoints to report detailed service status, including database connectivity, indexer status (for proxy-data), and provider availability (for proxy-core). This enables monitoring tools to detect when services are degraded.

User decision (from context): "Data Consistency: Stats can lag briefly during restarts - simpler architecture" implies we accept brief downtime, but want visibility.

### Acceptance Criteria

- [ ] proxy-core /health endpoint returns JSON with: {status: "ok", database: "connected", providers: ["anthropic", "openai"]}
- [ ] proxy-data /health endpoint returns JSON with: {status: "ok", database: "connected", indexer: "running"}
- [ ] Health check returns 503 (Service Unavailable) if database connection fails
- [ ] proxy-data health check returns 503 if indexer has crashed (aligns with fail-fast requirement)
- [ ] Optional: /metrics endpoint exposing Prometheus metrics (request count, latency)
- [ ] Documentation explains: "Use /health for load balancer health checks"

### Technical Notes

**Example health response (proxy-core):**
```json
{
  "status": "ok",
  "service": "proxy-core",
  "version": "1.0.0",
  "database": {
    "status": "connected",
    "path": "/data/requests.db"
  },
  "providers": {
    "anthropic": "available",
    "openai": "available"
  }
}
```

**Example health response (proxy-data):**
```json
{
  "status": "ok",
  "service": "proxy-data",
  "version": "1.0.0",
  "database": {
    "status": "connected",
    "path": "/data/requests.db"
  },
  "indexer": {
    "status": "running",
    "last_run": "2025-12-28T15:23:00Z",
    "conversations_indexed": 1234
  }
}
```

---

## P2-2: Verify Web Dashboard Works with Split Services

**Status:** Not Started
**Complexity:** Small
**Dependencies:** P1-1 (needs Caddy running)
**Spec Reference:** CLAUDE.md (Web Dashboard) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md section "Web dashboard changes"

### Description

Verify that the React web dashboard (web/) continues to work correctly when using the Caddy reverse proxy. All API calls should transparently route to the correct backend service without frontend code changes.

User decision (from context): "API Routing: Reverse proxy (nginx/caddy) - web apps unchanged"

### Acceptance Criteria

- [ ] Web dashboard starts successfully: `cd web && npm run dev`
- [ ] Dashboard fetches stats correctly from /api/stats (proxied to proxy-data)
- [ ] Request list loads correctly from /api/requests/summary
- [ ] Request detail view loads correctly from /api/requests/{id}
- [ ] Conversation search works correctly from /api/conversations
- [ ] No CORS errors in browser console
- [ ] No broken API calls (all 2xx responses)
- [ ] Performance is identical to monolith (no added latency from Caddy)
- [ ] Manual test: Send request via Claude Code ‚Üí appears in dashboard within 2 seconds

### Technical Notes

**Web app API base URL (should remain unchanged):**
```typescript
// web/app/utils/api.ts or similar
const API_BASE = import.meta.env.VITE_API_BASE || 'http://localhost:3000'
```

With Caddy on port 3000, all /api/* requests automatically route to proxy-data (port 3002), and the web app doesn't need to know about separate ports.

**Testing checklist:**
1. Start all services: `make dev` (or new command)
2. Open web dashboard: http://localhost:5173 (Vite dev server)
3. Verify request list loads
4. Click request detail - verify it loads
5. Search conversations - verify results appear
6. Check Network tab - all requests to localhost:3000, no errors

---

### P3 (Low): Nice-to-Have Improvements

---

## P3-1: Add Integration Tests for Service Split

**Status:** Not Started
**Complexity:** Medium
**Dependencies:** All P0, P1 items (needs full split working)
**Spec Reference:** CLAUDE.md (Testing) ‚Ä¢ **Status Reference:** STATUS-2025-12-28-144345.md section "Testing Strategy"

### Description

Create integration tests that verify the split architecture works correctly, including concurrent access to the shared SQLite database, service isolation, and end-to-end request flow.

### Acceptance Criteria

- [ ] Test: proxy-core can proxy requests while proxy-data is stopped (independence)
- [ ] Test: proxy-data can serve stats while proxy-core is stopped (independence)
- [ ] Test: Concurrent writes - proxy-core saves 100 requests while indexer processes conversations (no SQLite lock errors)
- [ ] Test: End-to-end flow - request via proxy-core appears in proxy-data stats within 2 seconds
- [ ] Test: Indexer fail-fast - proxy-data refuses to start if indexer initialization fails
- [ ] Test: Database path resolution - both services connect to same database with absolute path
- [ ] All tests pass in CI/CD pipeline
- [ ] Test documentation explains how to run: `go test ./integration/...`

### Technical Notes

**Test structure:**
```
proxy/
‚îî‚îÄ‚îÄ integration/
    ‚îú‚îÄ‚îÄ split_test.go        # Service isolation tests
    ‚îú‚îÄ‚îÄ database_test.go     # Concurrent access tests
    ‚îî‚îÄ‚îÄ e2e_test.go          # End-to-end flow tests
```

**Example test:**
```go
func TestProxyCoreIndependence(t *testing.T) {
    // Start only proxy-core
    core := startProxyCore(t)
    defer core.Stop()

    // Verify proxy-data is NOT running
    _, err := http.Get("http://localhost:3002/health")
    require.Error(t, err)

    // Send request to proxy-core
    resp := sendTestRequest(t, core)
    require.Equal(t, 200, resp.StatusCode)

    // Verify request was logged to database
    db := openDatabase(t)
    count := countRequests(db)
    require.Equal(t, 1, count)
}
```

**Run from STATUS file "Testing Strategy":**
- Baseline: Measure current throughput with wrk
- After split: Run same load test, compare latency
- Concurrent write test: Hammer proxy-core + trigger indexer simultaneously

---

## Dependency Graph

```
P0-1 (proxy-core)    P0-2 (proxy-data)    P0-3 (handler split)
      ‚îÇ                    ‚îÇ                     ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                      P1-1 (Caddy)
                           ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ                    ‚îÇ                    ‚îÇ
  P1-2 (Config)       P1-3 (Scripts)       P2-2 (Web verify)
                           ‚îÇ
                      P2-1 (Health checks)
                           ‚îÇ
                      P3-1 (Integration tests)
```

**Parallel work opportunities:**
- P0-1, P0-2, P0-3 can all be developed in parallel (independent work)
- P1-2 can be done anytime after P0-1/P0-2 (config shared by both)
- P2-1 can be done anytime after P0-1/P0-2 (enhances existing health endpoints)

**Critical path:**
1. P0-1 + P0-2 (required for any progress)
2. P1-1 (needed for web dashboard)
3. P1-3 (needed for easy deployment)

---

## Recommended Sprint Planning

### Sprint 1: Foundation (P0 items)

**Goal:** Prove the split works with minimal services running

**Work items:**
- P0-3: Split handlers first (enables clean code structure)
- P0-1: Build proxy-core binary
- P0-2: Build proxy-data binary

**Definition of Done:**
- Both binaries compile and run independently
- Manual test: proxy-core forwards request to Anthropic, logs to SQLite
- Manual test: proxy-data serves stats from existing SQLite data
- Manual test: Both running simultaneously without conflicts

**Estimated effort:** 1-2 days

---

### Sprint 2: Integration (P1 items)

**Goal:** Make the split usable with web dashboard

**Work items:**
- P1-1: Add Caddy reverse proxy
- P1-2: Update config for absolute DB path
- P1-3: Update build/run scripts

**Definition of Done:**
- `make dev` starts all services correctly
- Web dashboard loads and displays data
- README.md has clear instructions for running split services
- Can send Claude Code request ‚Üí appears in web dashboard

**Estimated effort:** 1 day

---

### Sprint 3: Polish (P2 items) - OPTIONAL

**Goal:** Production-ready monitoring and verification

**Work items:**
- P2-1: Enhanced health checks
- P2-2: Web dashboard verification

**Definition of Done:**
- Health endpoints return detailed status
- All web dashboard features work correctly
- Manual testing checklist completed and documented

**Estimated effort:** 0.5-1 day

---

## Risk Assessment

### High-Risk Items üî¥

**1. SQLite Write Conflicts**
- **Risk:** Both services writing to requests table simultaneously could cause lock timeouts
- **Likelihood:** Low (WAL mode handles this well per STATUS file)
- **Mitigation:** Already using WAL mode with 5000ms busy timeout. Test with concurrent writes.
- **Test plan:** Run P3-1 concurrent write test before deploying

**2. Web Dashboard API Routing**
- **Risk:** Dashboard breaks if API base URL is misconfigured
- **Likelihood:** Medium (requires Caddy setup)
- **Mitigation:** P1-1 (Caddy) is in critical path. P2-2 verifies dashboard.
- **Test plan:** Manual smoke test after P1-1 complete

**3. Indexer Failure Mode**
- **Risk:** Indexer crashes after startup, proxy-data continues running but search is broken
- **Likelihood:** Low (existing code is stable)
- **Mitigation:** User decision: fail-fast on indexer error. P0-2 implements this.
- **Test plan:** Kill indexer goroutine, verify proxy-data detects and reports unhealthy

### Medium-Risk Items ‚ö†Ô∏è

**4. Configuration Drift**
- **Risk:** Two binaries with different config expectations could diverge over time
- **Likelihood:** Medium (manual config management)
- **Mitigation:** P1-2 ensures both use same absolute DB path. Shared internal/ packages prevent API drift.
- **Test plan:** Integration test verifies both connect to same database

**5. Service Startup Order**
- **Risk:** Web dashboard loads before services are ready
- **Likelihood:** Low (services start quickly)
- **Mitigation:** P2-1 health checks enable readiness probes. run-split.sh waits for health checks.
- **Test plan:** Start services, wait for /health to return 200 before loading dashboard

### Low-Risk Items ‚úÖ

**6. Provider Code Migration**
- **Risk:** Provider code has hidden dependencies on data layer
- **Likelihood:** Very low (STATUS file confirms "zero dependencies on data layer")
- **Mitigation:** Provider code is self-contained. P0-1 includes full provider/ package.
- **Test plan:** Unit tests for provider code should pass unchanged

**7. Backward Compatibility**
- **Risk:** Existing API contracts break
- **Likelihood:** Very low (no API changes, just service split)
- **Mitigation:** P0-3 ensures no functional changes. Same handlers, just different binaries.
- **Test plan:** Run existing unit tests against both services

---

## Blockers and Questions

### Resolved (User Decisions Already Made)

‚úÖ **API Routing:** Reverse proxy (Caddy) - implemented in P1-1
‚úÖ **Data Consistency:** Eventual consistency acceptable (stats can lag during restarts)
‚úÖ **Indexer Criticality:** Fail-fast on error - implemented in P0-2
‚úÖ **Database Path:** Absolute path in config - implemented in P1-2

### Open Questions (Minor)

**Q1: Caddy vs. nginx?**
- **Context:** Both work identically for this use case
- **Recommendation:** Caddy (simpler config, automatic HTTPS if needed later)
- **Impact:** Low - can swap later if needed

**Q2: Service lifecycle management?**
- **Context:** Currently using run.sh scripts. Future: systemd? Docker Compose?
- **Recommendation:** Start with run.sh (P1-3), add systemd units or docker-compose.yml in future sprint
- **Impact:** Low - doesn't affect split implementation

**Q3: Logging strategy for split services?**
- **Context:** Currently all logs go to stdout. Should proxy-core and proxy-data have separate log files?
- **Recommendation:** Keep stdout for now, use systemd/Docker log drivers for separation in production
- **Impact:** Low - logging already uses separate prefixes ("proxy-core:", "proxy-data:")

---

## Success Metrics

**Technical Metrics:**
- ‚úÖ Both services compile and run independently
- ‚úÖ No SQLite lock errors under load (100 req/sec test)
- ‚úÖ Web dashboard functions identically to monolith
- ‚úÖ All existing unit tests pass
- ‚úÖ Integration tests pass (P3-1)

**Operational Metrics:**
- ‚úÖ Can restart proxy-data without dropping Claude Code requests
- ‚úÖ Can deploy proxy-data changes without touching proxy-core
- ‚úÖ Stats lag is < 2 seconds from request to dashboard display
- ‚úÖ Health checks accurately reflect service status

**Developer Experience Metrics:**
- ‚úÖ `make dev` starts all services correctly (one command)
- ‚úÖ README.md clearly explains how to run split architecture
- ‚úÖ Build time < 10 seconds for both binaries
- ‚úÖ Log output clearly distinguishes between services

---

## Notes

**Architecture Decision:** Phase 1 uses shared SQLite database (Option A from STATUS file). This is the simplest path to proving the split works. Future phases (P4+) can migrate to HTTP API (Option C) or message queue (Option B) if needed.

**Deferred to Future Sprints:**
- HTTP API between services (Phase 2)
- Message queue buffering (Phase 2)
- Multi-host deployment
- Docker Compose setup
- Kubernetes manifests
- Prometheus metrics integration

**Key Insight from STATUS File:**
> "The split is feasible and beneficial: Natural boundaries exist - code cleanly separates into proxy vs. data concerns."

This plan focuses on the MINIMAL viable split (Phase 1) to achieve the core goal: proxy-core stability and proxy-data iteration independence.
